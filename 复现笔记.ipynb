{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c7ca0df",
   "metadata": {},
   "source": [
    "<img src=\"readme.png\" alt=\"Schematic diagram of DigNet generation network\" />\n",
    "\n",
    "## 1.输入数据\n",
    "### 1.1 \n",
    "基因表达谱和GRN(数据格式见`Supplement S1`)\n",
    "\n",
    "- **'net' variable**: Contains the adjacency matrix of network data, a 0-1 weight matrix (Numpy ndarray). Non-0-1 values can also be loaded, sized cell * cell.\n",
    "- **'exp' variable**: Contains experimental data in DataFrame format, preprocessed scRNA-seq results (CSV format), sized gene * cell.\n",
    "\n",
    "\n",
    "### 1.2\n",
    "只输入基因表达谱, 有方法构建一个参考网络(见manuscript, 即readme文档使用的输入格式), 接下来都是使用该格式输入\n",
    "\n",
    "预处理:参考`CancerDatasets/Create_BRCA_data.py`,基因表达谱是一个csv或xlsx文件,第一行是cell numbers, 第一列是gene symbol IDs\n",
    "\n",
    "\n",
    "\n",
    "## 2.文件和文件夹\n",
    "\n",
    "- `pre_train/`: The pre-trained DigNet model provided in this article\n",
    "- `Cancer_datasets/`: Contains preprocessed results of the E-MTAB-8107 data used in the manuscript\n",
    "- `pathway/`: sub-function file containing data preprocessing\n",
    "\n",
    "> 预处理\n",
    "> MaxMinNormalization(x, Min=0, Max=1): 归一化到(0,1)区间\n",
    ">\n",
    "> cal_percent(new_bit_crop,corr_TF_Gene, MI_TF_Gene,net_bit_orig):\n",
    "1.首先new_bit_origC赋值为net_bit_orig和的new_bit_crop交集,然后计算在new_bit_crop中的比例,记为NUM_ORIG\n",
    "2.corr_TF_GeneC赋值为corr_TF_GeneC和new_bit_crop的交集,然后计算与net_bit_origC不相交的个数在new_bit_crop中的比例,记为NUM_PCC\n",
    "3.对MI_TF_Gene的操作与corr_TF_GeneC相同,得到NUM_MI\n",
    "4.如果NUM_ORIG,NUM_PCC,NUM_MI相加不是100,则缩放使得和为100\n",
    "5.NUM_PCC+NUM_MIover>50 -> overflow = True\n",
    "else: overflow = False\n",
    "return NUM_ORIG, NUM_PCC, NUM_MI, overflow\n",
    "\n",
    ">load_sergio_count(filename='pathway/simulation/SERGIO_data_node_2000.data',  num=None, logp=True):\n",
    "用pickle加载filename,如果有num,则选择索引为num的数据,如果logp == True, 则进行log1p变换\n",
    "return batch\n",
    "\n",
    ">plot_GRN_percent(network_percent):\n",
    "画一个\"Train_GRN_pecent_bar.pdf\"柱形图,\n",
    "以pathway为横坐标,\n",
    "NUM_ORIG,NUM_PCC,NUM_MI为纵坐标\n",
    "\n",
    ">compute_mutual_information(df):\n",
    "df每行是一个特征\n",
    "计算df特征间的的mi(互信息)\n",
    "return mi_matrix\n",
    "\n",
    ">compare_char(charlist, setlist):\n",
    "index为setlist中charlist的索引\n",
    "一般来说index方法只能在list中找一个值的索引,所以charlist可能不是list类型,只是一个值\n",
    "return index\n",
    "\n",
    ">calRegnetwork(human_network, GRN_GENE_symbol):\n",
    "human_network_TF_symbol = human_network第0列\n",
    "human_network_Gene_symbol= human_network第2列\n",
    "GRN_GENE_symbol中找调控基因(TF)和靶基因(Gene)\n",
    "这一段循环方式低效,应该可以改进\n",
    "return pd.DataFrame(network, columns=['TF', 'Gene'])\n",
    "\n",
    ">load_KEGG(kegg_file='pathway/kegg/KEGG_all_pathway.pkl'):\n",
    "如果 pkl 文件存在，则加载它,如果 pkl 文件不存在，则运行 KEGG.py 文件\n",
    "\n",
    ">high_MI(exp_pca_discretized, exp_pca, net_bit, parm):\n",
    "计算exp_pca_discretized的互信息,保存在row_MI中,将row_MI的对角线设置为0.然后筛选大于阈值的调控基因和靶基因,把调控基因和靶基因保存到MI_TF_Gene中,并且与net_bit拼接去重.\n",
    "return net_bit, MI_TF_Gene\n",
    "\n",
    ">high_pearson(exp_pca, net_bit, parm):\n",
    "与high_MI中的操作类似,不过未经过离散化.\n",
    "值得注意的是,mi和pearson相关系数都是对称的,也就是说调控基因和靶基因(有向边)并没有明确的区分?\n",
    "\n",
    ">from_cancer_create(BRCA_exp_filter_saver, KEGG, parm, lim=200, test_pathway=None, Other_Pathway=None, human_network=None):\n",
    "1.获得exp表达谱\n",
    "2.对exp进行标准化\n",
    "3.net_bit,用calRegnetwork函数在human_network的先验信息中得到,由两列组成:调控基因TF和靶基因Gene\n",
    "4.net_bit_orig:net_bit的拷贝\n",
    "5.exp_pca:通过pca降维,列数比exp少一\n",
    "6.将exp_pca分成256个离散化的区间(0-255),保存为exp_pca_discretized\n",
    "7.用上面两个函数计算高pearson和MI的调控-靶基因,添加到net_bit\n",
    "8.创建邻接矩阵adj_matrix\n",
    "8.1 矩阵形状:基因个数(exp.index) * 基因个数\n",
    "8.2 有调控关系的(TF-Gene)在矩阵中置1\n",
    "8.3 经过cal_del_TF_edge(exp.index)函数,将exp分成TF,GENE两个不相交的部分\n",
    "然后在邻接矩阵adj_matrix中将下标为GENE,TF的点置0(这样就从无向图变成了有向图)\n",
    "9.predicted_adj_matrix, new_graph = pca_cmi(exp_pca_discretized, net_bit, parm['pmi_percent'], 1):\n",
    "参考**其他文件**,通过删除cmi较小的边,获得邻接矩阵和图.此处有重要的问题,即离散化的数据用多元正态分布的方法计算cmi不合理.\n",
    "new_bit_crop是更新后的图的所有边\n",
    "10.以上有两个邻接矩阵adj_matrix,predicted_adj_matrix.前者是用pearson和mi计算的,后者是用cmi计算的,以下要对两者进行比较\n",
    "11.分为三种情况,选择边最少的邻接矩阵\n",
    "11.1 predicted_adj_matrix所有值为0\n",
    "11.2 (np.sum(adj_matrix) / np.sum(predicted_adj_matrix)) < 0.5:\n",
    "return exp, adj_matrix, new_row\n",
    "11.3 else:\n",
    "return exp, predicted_adj_matrix, new_row\n",
    "评价:这里用0.5比较,是因为adj_matrix是有向图\n",
    "predicted_adj_matrix还是一个对称的图,所以如果两个有向图的边相同,predicted_adj_matrix会多一倍的边.但是如果选了predicted_adj_matrix,为什么不把他从对称矩阵(无向)变成有向图?\n",
    "\n",
    ">matrix2Data(adj_matrix, node_feature, num=0, adj2data=True, log_trans=True, metacell=False, Cnum=100, k=20):\n",
    "1.1如果node_feature是list\n",
    "node_feature中选择第num个,转换为tensor,得到x\n",
    "1.2如果node_feature不是list\n",
    "直接将node_feature转换为tensor,得到x\n",
    "log_trans默认True,进行对数变换x = log(1+x)\n",
    "2.归一化:使用MinMaxScalar,得到x_normalized\n",
    "3.1如果adj2data是True(默认为True),此时邻接矩阵是0-1编码的,即表示无权图\n",
    "adj_matrix转换为tensor\n",
    "indices_tensor是所有的边,每一列代表一条边的头节点和尾节点\n",
    "num_edges是边的数量\n",
    "values_tensor是一个全为1的张量,长度是边的数量\n",
    "3.2如果adj2data是False,此时邻接矩阵是一个稠密的,图中每个点(i,j)都有权重的数组\n",
    "indeces = 邻接矩阵的所有索引,每一列代表一条边的头节点和尾节点\n",
    "values = 邻接矩阵的所有值\n",
    "然后转换成张量,indeces_tensor,values_tensor\n",
    "4.创建data,一个图,节点特征为x_normalized(形状为(节点数,特征数)),边的索引和权重分别是indeces_tensor,values_tensor,索引为0-(节点数-1)\n",
    "return data\n",
    "\n",
    ">load_pathway_mat(file_path, num=0):\n",
    "从mat类型数据中读取data\n",
    "node_feature是8个节点的特征,这个8是怎么确定的?\n",
    "adj_matrix是data中的邻接矩阵\n",
    "Data1 = matrix2Data(adj_matrix, node_feature, num=num)创建图\n",
    "return Data1\n",
    "\n",
    ">create_batch_dataset_from_mat(matnum=100, test=False):\n",
    "1.如果test = True:测试集\n",
    "直接加载matnum的mat文件到batch\n",
    "2.否则:训练集\n",
    "加载1-matnum的所有mat文件到batch\n",
    "return batch\n",
    "\n",
    ">create_batch_dataset_simu(filename='./pathway/simulation/SERGIO_data_node_2000.data', num=None, device=None,test=False,adddata=None, metacell=True, Cnum=100, k=20):\n",
    "\n",
    "\n",
    "- `pathway/simulation/`: synthetic network and gene expression profile generated by SERGIO\n",
    "- `discrete/`: model-related configuration files\n",
    "- `discrete/models/`: Graph Transformer architecture\n",
    "- `denoising_diffusion_pytorch/`: Contains related sub-function files used for training/testing/initializing DigNet\n",
    "- `config.py`: Configure hyperparameters for training or testing DigNet\n",
    "- `DigNet.py`: Contains the process framework of DigNet\n",
    "- `Download_TF_file.py`: used to download TF list\n",
    "- `make_final_net.py`: Integrated voting sub-function\n",
    "- `Tutorial.py`: A quick tutorial for using DigNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f266246",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d9789d06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1,\n",
       " array([[0, 1, 2, 3, 4],\n",
       "        [5, 6, 7, 8, 9]]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = np.arange(10).reshape(2,-1)\n",
    "df[0,1],df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c128b276",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n",
      "{'b'}\n",
      "b\n",
      "{'c'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 1, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0]], dtype=int32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import networkx as nx\n",
    "graph = nx.DiGraph()\n",
    "graph.add_edges_from([['a','b'],['b','c']])\n",
    "\n",
    "for edge in list(graph.edges()):\n",
    "    print(edge[0])\n",
    "    print(set(graph.neighbors(edge[0])))\n",
    "\n",
    "graph.add_nodes_from([2,2,3,3,4,4,5])\n",
    "nx.adjacency_matrix(graph).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "43e0b483",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 2, 3), (1, 2, 4), (1, 3, 4), (2, 3, 4)]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import itertools\n",
    "a = [1,2,3,4]\n",
    "list(itertools.combinations(a, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ca3662",
   "metadata": {},
   "source": [
    "## 其他文件\n",
    "pathway/PCA_CMI.py\n",
    ">conditional_mutual_info(X, Y, Z=np.array(1)):\n",
    "计算条件互信息cmi,多元正态分布\n",
    "如果X,Y是1维,修改形状为(-1,1)的2维数组.此处应该有误,因为cov是将每行当作特征,每列当作样本,(-1,1)的形状算不出协方差,(1,-1)的形状才能算出单个向量的方差.这里没出现报错,输入X,Y应该都是2维数组,列数相同(才能进行cov计算)\n",
    "Z是常数或向量,用矩阵来说是0维或1维\n",
    "分以下两种情况:\n",
    "1.Z是0维,即Z不是一个条件,cmi退化成mi\n",
    "d1,d2分别是X,Y的协方差的行列式\n",
    "d3是X和Y拼接后的协方差的行列式\n",
    "cmi = (1 / 2) * np.log((d1 * d2) / d3)\n",
    "2.Z是1维,即Z是条件\n",
    "d1是X,Z的协方差的行列式\n",
    "d2是Y,Z的协方差的行列式\n",
    "d3是Z的协方差的行列式\n",
    "d4是X,Y,Z的协方差的行列式\n",
    "cmi = (1 / 2) * np.log((d1 * d2) / (d3 * d4))\n",
    "关于公式推导见**补充知识**\n",
    "return cmi\n",
    "\n",
    ">remove_edges(predicted_graph, data, L, theta):\n",
    "先从predicted_graph中获得边的数量initial_num_edges和所有边edges\n",
    "遍历每条边edge:\n",
    "neighbors1 = edge[0]的子节点集合\n",
    "neighbors2 = edge[1]的子节点集合\n",
    "neighbors = 以上两个集合取交集,即两个点的公共子节点集合\n",
    "nhbrs = neighbors的拷贝\n",
    "T = 公共子节点个数\n",
    "如果T < L(阈值)或者edge的两个点相同(即edge是自环边),不进行操作\n",
    "否则:\n",
    "在data中根据edge的两个端点找到x,y,他们的形状是(-1,1)\n",
    "K = 在nhbrs中选择L个的组合\n",
    "1.如果L == 0:\n",
    "cmiVal = conditionconditional_mutual_info(x.T, y.T)\n",
    "这个转置(即(1,-1)的输入)也跳过了上面conditionconditional_mutual_info中不合理的判别\n",
    "如果cmiVal < theta则删除这条边\n",
    "2.否则:遍历K中的zgroup\n",
    "2.1 zgroup中没有数据,则跳过\n",
    "2.2 data中通过zgroup查找z,即影响xy互信息的条件\n",
    "cmiVal = conditional_mutual_info(x.T, y.T, z.T)\n",
    "在对zgroup的遍历中获得最大的cmi,如果小于theta则删除这条边\n",
    "在对每条边进行判断是否删除的操作后,计算图中边的数量,如果比输入时减少了,return predicted_graph, False\n",
    "否则return predicted_graph, True\n",
    "\n",
    ">pca_cmi(data, new_net_bit, theta, max_order, show=False):\n",
    "创建有向图predicted_graph,节点为data的index\n",
    "new_net_bit中的两列TF,Gene分别是每条边的头节点,尾节点\n",
    "L = 0\n",
    "nochange = False\n",
    "data = data.T\n",
    "循环,不断增加L,直到nochange == True\n",
    "这个循环隐含的假设是,随着L增大,条件越多,cmi值会越小,导致不断有边被删除,直到所有边的端点的cmi大于theta,不能被删除.\n",
    "但是查找资料:条件越多,条件互信息不一定越小或越大,其变化取决于条件变量与目标变量之间的因果或依赖结构.也就是说cmi不是一个单调的结构,这一段也需要考察.\n",
    "调用时L = 1,即删除操作只进行一次,也不用考察L更大时的单调性质了.\n",
    "show参数默认是False,如果为True有一段打印信息,调试时可以使用.\n",
    "返回邻接矩阵和图结构\n",
    "return predicted_adjMatrix, predicted_graph\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30aedbd1",
   "metadata": {},
   "source": [
    "## 补充知识\n",
    "### 互信息(mi)和条件互信息(cmi)\n",
    "熵,联合熵,条件熵的定义\n",
    "$H(x) = -\\sum_x p(x)log(p(x))\\\\\n",
    "H(x,y) = -\\sum_{x}\\sum_{y} p(x,y)log(p(x,y))\\\\\n",
    "H(x|y) = H(x,y) - H(y)$ \n",
    "首先,证明互信息mi的熵表达式和概率表达式等价\n",
    "`熵表达式`\n",
    "$mi(x,y) = H(x) + H(y) - H(x,y)\\\\\n",
    "=H(x)-H(x|y)= H(y) - H(y|x)$\n",
    "`概率表达式`\n",
    "$mi(x,y) = \\sum_{x}\\sum_{y} p(x,y)log(\\frac{p(x,y)}{p(x)p(y)})$\n",
    "证明:\n",
    "$$\\begin{align*}\n",
    "mi(x,y) &= H(x) + H(y) - H(x,y) \\\\\n",
    "&= -\\sum_x p(x)log(p(x))-\\sum_y p(y)log(p(y))+\\sum_{x}\\sum_{y} p(x,y)log(p(x,y)) \\\\\n",
    "&= -\\sum_x\\sum_{y} p(x,y)log(p(x))-\\sum_{x}\\sum_y p(x,y)log(p(y))+\\sum_{x}\\sum_{y} p(x,y)log(p(x,y)) \\\\\n",
    "&=\\sum_{x}\\sum_{y} p(x,y)log(\\frac{p(x,y)}{p(x)p(y)})\n",
    "\\end{align*}$$\n",
    "\n",
    "条件互信息cmi\n",
    "$cmi(x,y|z) =  H(x|z) + H(y|z) - H(x,y|z)\\\\ = \\sum_zp(z)mi(x,y|z)$\n",
    "\n",
    "关于本文中计算cmi的方法,网上的解释:\n",
    "条件互信息（CMI）与行列式之间存在一定的联系，但直接使用行列式计算CMI并不常见，需要根据具体情况分析。下面从理论和实际应用两个角度进行说明：\n",
    "\n",
    "\n",
    "### **1. 理论上的联系**\n",
    "对于**多元高斯分布**，CMI可以通过协方差矩阵的行列式计算。假设随机变量 \\( X, Y, Z \\) 服从联合高斯分布，则：\n",
    "\\[\n",
    "\\text{CMI}(X;Y|Z) = \\frac{1}{2} \\ln \\left( \\frac{\\det(\\Sigma_{XZ}) \\cdot \\det(\\Sigma_{YZ})}{\\det(\\Sigma_{Z}) \\cdot \\det(\\Sigma_{XYZ})} \\right)\n",
    "\\]\n",
    "其中：\n",
    "- \\( \\Sigma_{XZ} \\) 是 \\( X \\) 和 \\( Z \\) 的协方差矩阵\n",
    "- \\( \\Sigma_{YZ} \\) 是 \\( Y \\) 和 \\( Z \\) 的协方差矩阵\n",
    "- \\( \\Sigma_{Z} \\) 是 \\( Z \\) 的协方差矩阵\n",
    "- \\( \\Sigma_{XYZ} \\) 是 \\( X, Y, Z \\) 的联合协方差矩阵\n",
    "\n",
    "**推导思路**：\n",
    "高斯分布的微分熵为 \\( H(X) = \\frac{1}{2} \\ln \\left( (2\\pi e)^n \\det(\\Sigma_X) \\right) \\)，代入CMI的熵表达式：\n",
    "\\[\n",
    "\\text{CMI}(X;Y|Z) = H(X|Z) + H(Y|Z) - H(X,Y|Z)\n",
    "\\]\n",
    "通过行列式的比值化简可得上述公式。\n",
    "\n",
    "\n",
    "### **2. 离散数据的CMI与行列式**\n",
    "对于**离散数据**，CMI的定义基于概率分布，而非协方差矩阵，因此**无法直接使用行列式计算**。例如：\n",
    "\\[\n",
    "\\text{CMI}(X;Y|Z) = \\sum_{x,y,z} P(x,y,z) \\log \\frac{P(x,y|z)}{P(x|z) \\cdot P(y|z)}\n",
    "\\]\n",
    "此时需要通过统计频率估计概率分布，而非行列式。\n",
    "\n",
    "### 总结\n",
    "本文假设变量都服从多元高斯分布"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c13551",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.7388948450374015, 0.04718490304900292)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#计算互信息的代码,可以看出用两种方法得到的mi差别很大\n",
    "#注,mutual_info_score仅适用于离散数据,基于行列式的计算方法仅适用于多元高斯分布,\n",
    "#所以两种方法可比性不高,但是本文中两种方式都出现了,需要考察\n",
    "from sklearn.metrics import mutual_info_score\n",
    "x = np.random.randn(20)\n",
    "y = np.random.randn(20)\n",
    "#离散化\n",
    "bins = np.linspace(min(x),max(x),21)\n",
    "bins1 = np.linspace(min(y),max(y),21)\n",
    "labels = list(range(20))\n",
    "x_discrete = pd.cut(x,bins = bins,labels = labels,include_lowest=True)\n",
    "y_discrete = pd.cut(y,bins = bins1,labels = labels,include_lowest=True)\n",
    "mi1 = mutual_info_score(x_discrete.to_list(),y_discrete.to_list())\n",
    "d1 = np.cov(x_discrete)\n",
    "d2 = np.cov(y_discrete)\n",
    "d3 = np.linalg.det(np.cov(x_discrete,y_discrete))\n",
    "mi2 = 1/2 * np.log((d1 * d2)/d3)\n",
    "mi1, mi2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1818c5e8",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dignet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
